"""
AI ëŒ€ë³¸ ìƒì„± API ë¼ìš°í„°
OpenAI API + ê¸°ì¡´ ëŒ€ë³¸/í‰ê°€ë¥¼ ì°¸ì¡°í•˜ì—¬ ì§„í™”í•˜ëŠ” ëŒ€ë³¸ ìƒì„±
"""

import os
from typing import List, Optional
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from pydantic import BaseModel

from models.database import get_db
from models.database.crud import (
    get_project, get_episode, get_characters_by_project,
    get_scene, get_scenes_by_episode, get_evaluation_by_scene
)
from models.database.models import SceneModel, EvaluationModel, EpisodeModel

router = APIRouter()


class GenerateRequest(BaseModel):
    """ëŒ€ë³¸ ìƒì„± ìš”ì²­"""
    scene_id: int
    goal: str
    scene_type: str = "dialogue"
    conflict_type: str = "none"
    character_ids: List[int] = []
    additional_context: Optional[str] = None


class GenerateResponse(BaseModel):
    """ëŒ€ë³¸ ìƒì„± ì‘ë‹µ"""
    scene_id: int
    content: str
    word_count: int
    success: bool
    context_used: dict  # ì–´ë–¤ ì»¨í…ìŠ¤íŠ¸ê°€ ì‚¬ìš©ë˜ì—ˆëŠ”ì§€


def get_openai_client():
    """OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        return None
    
    try:
        from openai import OpenAI
        return OpenAI(api_key=api_key)
    except ImportError:
        return None


def build_learning_context(db: Session, project_id: int, current_scene_id: int = None):
    """
    í”„ë¡œì íŠ¸ì˜ ê¸°ì¡´ ëŒ€ë³¸ê³¼ í‰ê°€ë¥¼ ë¶„ì„í•˜ì—¬ í•™ìŠµ ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶•
    
    Returns:
        dict: í•™ìŠµëœ ì»¨í…ìŠ¤íŠ¸ ì •ë³´
    """
    context = {
        "best_scenes": [],       # ë†’ì€ ì ìˆ˜ ë°›ì€ ì¥ë©´ë“¤
        "style_patterns": [],    # ìŠ¤íƒ€ì¼ íŒ¨í„´
        "strengths_to_keep": [], # ìœ ì§€í•´ì•¼ í•  ê°•ì 
        "issues_to_avoid": [],   # í”¼í•´ì•¼ í•  ë¬¸ì œì 
        "character_examples": {},# ìºë¦­í„°ë³„ ëŒ€ì‚¬ ì˜ˆì‹œ
        "scene_count": 0,
        "avg_score": 0
    }
    
    # í”„ë¡œì íŠ¸ì˜ ëª¨ë“  ì—í”¼ì†Œë“œ ê°€ì ¸ì˜¤ê¸°
    episodes = db.query(EpisodeModel).filter(
        EpisodeModel.project_id == project_id
    ).all()
    
    all_scenes = []
    all_evaluations = []
    
    for ep in episodes:
        scenes = get_scenes_by_episode(db, ep.id)
        for scene in scenes:
            if current_scene_id and scene.id == current_scene_id:
                continue  # í˜„ì¬ ìƒì„± ì¤‘ì¸ ì¥ë©´ ì œì™¸
            
            if scene.content and len(scene.content) > 50:  # ìœ íš¨í•œ ëŒ€ë³¸ë§Œ
                all_scenes.append(scene)
                
                # í‰ê°€ ê°€ì ¸ì˜¤ê¸°
                evaluation = get_evaluation_by_scene(db, scene.id)
                if evaluation:
                    all_evaluations.append({
                        "scene": scene,
                        "evaluation": evaluation
                    })
    
    context["scene_count"] = len(all_scenes)
    
    if not all_evaluations:
        return context
    
    # í‰ê·  ì ìˆ˜ ê³„ì‚°
    scores = [e["evaluation"].overall_score for e in all_evaluations]
    context["avg_score"] = sum(scores) / len(scores) if scores else 0
    
    # ìƒìœ„ 3ê°œ ì¥ë©´ ì„ íƒ (ê°€ì¥ ë†’ì€ ì ìˆ˜)
    sorted_evals = sorted(all_evaluations, key=lambda x: x["evaluation"].overall_score, reverse=True)
    top_scenes = sorted_evals[:3]
    
    for item in top_scenes:
        scene = item["scene"]
        eval = item["evaluation"]
        
        # ëŒ€ë³¸ ì¼ë¶€ ì¶”ì¶œ (ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ)
        content_preview = scene.content[:500] if len(scene.content) > 500 else scene.content
        
        context["best_scenes"].append({
            "scene_type": scene.scene_type,
            "score": eval.overall_score,
            "content_preview": content_preview
        })
        
        # ê°•ì  ìˆ˜ì§‘
        if eval.strengths:
            context["strengths_to_keep"].extend(eval.strengths)
    
    # ëª¨ë“  í‰ê°€ì—ì„œ í”¼í•´ì•¼ í•  ì  ìˆ˜ì§‘
    for item in all_evaluations:
        eval = item["evaluation"]
        if eval.suggestions:
            context["issues_to_avoid"].extend(eval.suggestions[:2])  # ìƒìœ„ 2ê°œë§Œ
        if eval.cliche_detected and eval.cliches:
            for cliche in eval.cliches[:2]:
                if isinstance(cliche, dict):
                    context["issues_to_avoid"].append(f"í´ë¦¬ì…° í”¼í•˜ê¸°: {cliche.get('description', '')}")
    
    # ì¤‘ë³µ ì œê±°
    context["strengths_to_keep"] = list(set(context["strengths_to_keep"]))[:5]
    context["issues_to_avoid"] = list(set(context["issues_to_avoid"]))[:5]
    
    # ìºë¦­í„°ë³„ ëŒ€ì‚¬ ì˜ˆì‹œ ì¶”ì¶œ
    for scene in all_scenes[:5]:  # ìµœê·¼ 5ê°œ ì¥ë©´ì—ì„œ
        if scene.content:
            lines = scene.content.split('\n')
            for line in lines:
                if ':' in line:
                    parts = line.split(':', 1)
                    char_name = parts[0].strip()
                    dialogue = parts[1].strip() if len(parts) > 1 else ""
                    
                    if char_name and dialogue and len(dialogue) > 10:
                        if char_name not in context["character_examples"]:
                            context["character_examples"][char_name] = []
                        if len(context["character_examples"][char_name]) < 2:
                            context["character_examples"][char_name].append(dialogue[:100])
    
    return context


@router.post("/scene", response_model=GenerateResponse, summary="AI ëŒ€ë³¸ ìƒì„±")
async def generate_scene_content(
    request: GenerateRequest,
    db: Session = Depends(get_db)
):
    """
    AIë¥¼ ì‚¬ìš©í•˜ì—¬ ì¥ë©´ì˜ ëŒ€ë³¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    ê¸°ì¡´ ëŒ€ë³¸ê³¼ í‰ê°€ë¥¼ í•™ìŠµí•˜ì—¬ ì¼ê´€ëœ ìŠ¤íƒ€ì¼ê³¼ ê°œì„ ëœ í’ˆì§ˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    """
    # ì¥ë©´ ì¡°íšŒ
    scene = get_scene(db, request.scene_id)
    if not scene:
        raise HTTPException(status_code=404, detail="ì¥ë©´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ì—í”¼ì†Œë“œ & í”„ë¡œì íŠ¸ ì •ë³´
    episode = get_episode(db, scene.episode_id)
    if not episode:
        raise HTTPException(status_code=404, detail="ì—í”¼ì†Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    project = get_project(db, episode.project_id)
    if not project:
        raise HTTPException(status_code=404, detail="í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ìºë¦­í„° ì •ë³´
    all_characters = get_characters_by_project(db, project.id)
    selected_characters = [c for c in all_characters if c.id in request.character_ids]
    
    # ğŸ”¥ í•µì‹¬: ê¸°ì¡´ ëŒ€ë³¸/í‰ê°€ì—ì„œ í•™ìŠµ ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶•
    learning_context = build_learning_context(db, project.id, scene.id)
    
    # OpenAI í´ë¼ì´ì–¸íŠ¸ í™•ì¸
    client = get_openai_client()
    
    if client:
        # OpenAIë¡œ ì‹¤ì œ ìƒì„± (í•™ìŠµ ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
        content = await generate_with_openai(
            client=client,
            project=project,
            episode=episode,
            scene=scene,
            characters=selected_characters,
            goal=request.goal,
            scene_type=request.scene_type,
            conflict_type=request.conflict_type,
            learning_context=learning_context
        )
    else:
        # OpenAI ì—†ìœ¼ë©´ ìƒ˜í”Œ ëŒ€ë³¸ ìƒì„±
        content = generate_sample_script(
            project=project,
            characters=selected_characters,
            goal=request.goal,
            scene_type=request.scene_type,
            learning_context=learning_context
        )
    
    # DB ì—…ë°ì´íŠ¸
    scene.content = content
    scene.word_count = len(content)
    scene.ai_generated = True
    scene.goal = request.goal
    # titleì´ ì—†ìœ¼ë©´ goalì˜ ì²« ì¤„ì„ titleë¡œ ì„¤ì •
    if not scene.title or scene.title == "":
        scene.title = request.goal.split('\n')[0][:50] if request.goal else f"ì¥ë©´ {scene.scene_number}"
    db.commit()
    
    # ğŸ”¥ ìë™ í‰ê°€ ìƒì„±
    evaluation_result = await auto_evaluate_scene(
        db=db,
        client=client,
        scene=scene,
        content=content,
        learning_context=learning_context
    )
    
    return GenerateResponse(
        scene_id=scene.id,
        content=content,
        word_count=len(content),
        success=True,
        context_used={
            "scenes_referenced": learning_context["scene_count"],
            "avg_score": round(learning_context["avg_score"], 2),
            "strengths_applied": len(learning_context["strengths_to_keep"]),
            "issues_avoided": len(learning_context["issues_to_avoid"]),
            "auto_evaluation": evaluation_result
        }
    )


async def generate_with_openai(client, project, episode, scene, characters, goal, scene_type, conflict_type, learning_context):
    """OpenAI APIë¡œ ëŒ€ë³¸ ìƒì„± (í•™ìŠµ ì»¨í…ìŠ¤íŠ¸ ë°˜ì˜)"""
    
    # ìºë¦­í„° ì •ë³´ í¬ë§·
    char_info_parts = []
    for c in characters:
        char_text = f"- {c.name} ({c.role}): {c.personality_description or ''}"
        if c.speech_pattern:
            char_text += f"\n  ë§íˆ¬: {c.speech_pattern}"
        
        # ê¸°ì¡´ ëŒ€ì‚¬ ì˜ˆì‹œ ì¶”ê°€
        if c.name in learning_context.get("character_examples", {}):
            examples = learning_context["character_examples"][c.name]
            char_text += f"\n  ì‹¤ì œ ëŒ€ì‚¬ ì˜ˆì‹œ: " + " / ".join(examples)
        elif c.speech_examples:
            char_text += f"\n  ëŒ€ì‚¬ ì˜ˆì‹œ: " + " / ".join(c.speech_examples[:2])
        
        char_info_parts.append(char_text)
    
    char_info = "\n".join(char_info_parts) if char_info_parts else "ìºë¦­í„° ì •ë³´ ì—†ìŒ"
    
    # ì¥ë©´ íƒ€ì… ì„¤ëª…
    type_desc = {
        "opening": "ì˜¤í”„ë‹ - ì‹œì²­ìì˜ ê´€ì‹¬ì„ ëŒê³  ì£¼ì œë¥¼ ì†Œê°œ",
        "dialogue": "ëŒ€í™” ì¥ë©´ - ìºë¦­í„° ê°„ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”",
        "talk": "ë³¸ê²© í† í¬ - ì£¼ì œì— ëŒ€í•œ ì‹¬ì¸µ í† ë¡ ",
        "highlight": "í•˜ì´ë¼ì´íŠ¸ - í•µì‹¬ í¬ì¸íŠ¸, ì„íŒ©íŠ¸ ìˆëŠ” ì¥ë©´",
        "closing": "ë§ˆë¬´ë¦¬ - ì •ë¦¬ì™€ ë‹¤ìŒ íšŒ ì˜ˆê³ "
    }.get(scene_type, "ëŒ€í™” ì¥ë©´")
    
    # ê°ˆë“± ìœ í˜• ì„¤ëª…
    conflict_desc = {
        "none": "ê°ˆë“± ì—†ìŒ - í¸ì•ˆí•œ ë¶„ìœ„ê¸°",
        "relationship": "ê´€ê³„ ê°ˆë“± - ì¸ë¬¼ ê°„ ê¸´ì¥",
        "internal": "ë‚´ë©´ ê°ˆë“± - ê³ ë¯¼, ë”œë ˆë§ˆ",
        "ideological": "ê°€ì¹˜ê´€ ê°ˆë“± - ì˜ê²¬ ëŒ€ë¦½",
        "comedic": "ì½”ë¯¹ ê°ˆë“± - ìœ ë¨¸ëŸ¬ìŠ¤í•œ ëŒ€ë¦½"
    }.get(conflict_type, "")
    
    # ğŸ”¥ í•™ìŠµ ì»¨í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    learning_prompt = ""
    
    if learning_context["scene_count"] > 0:
        learning_prompt = f"""
## ğŸ¯ ì´ í”„ë¡œì íŠ¸ì˜ í•™ìŠµëœ ìŠ¤íƒ€ì¼ (ê¸°ì¡´ {learning_context['scene_count']}ê°œ ì¥ë©´ ë¶„ì„)

### ìœ ì§€í•´ì•¼ í•  ê°•ì  (ì´ì „ ëŒ€ë³¸ì—ì„œ ë†’ì€ í‰ê°€ë¥¼ ë°›ì€ ìš”ì†Œ):
{chr(10).join(['- ' + s for s in learning_context['strengths_to_keep']]) if learning_context['strengths_to_keep'] else '- ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” íë¦„'}

### í”¼í•´ì•¼ í•  ì  (ì´ì „ í”¼ë“œë°±ì—ì„œ ì§€ì ëœ ë¬¸ì œ):
{chr(10).join(['- ' + s for s in learning_context['issues_to_avoid']]) if learning_context['issues_to_avoid'] else '- ì§„ë¶€í•œ í‘œí˜„'}

### ì°¸ê³ í•  ì¢‹ì€ ëŒ€ë³¸ ì˜ˆì‹œ (ì ìˆ˜ {learning_context['avg_score']:.0%} ì´ìƒ):
"""
        for i, best in enumerate(learning_context["best_scenes"][:2], 1):
            learning_prompt += f"""
ì˜ˆì‹œ {i} ({best['scene_type']}, ì ìˆ˜: {best['score']:.0%}):
```
{best['content_preview'][:300]}...
```
"""

    prompt = f"""ë‹¹ì‹ ì€ ì „ë¬¸ ëŒ€ë³¸ ì‘ê°€ì…ë‹ˆë‹¤. ê¸°ì¡´ ëŒ€ë³¸ì˜ ìŠ¤íƒ€ì¼ê³¼ í”¼ë“œë°±ì„ í•™ìŠµí•˜ì—¬ ë” ë‚˜ì€ ëŒ€ë³¸ì„ ì‘ì„±í•©ë‹ˆë‹¤.

## í”„ë¡œì íŠ¸ ì •ë³´
- ì œëª©: {project.title}
- íƒ€ì…: {project.project_type}
- ì„¸ê³„ê´€: {project.world_setting or 'ì—†ìŒ'}
- ìŠ¤íƒ€ì¼ ê°€ì´ë“œ: {project.style_guide or 'ì—†ìŒ'}

## ì—í”¼ì†Œë“œ ì •ë³´
- EP{episode.episode_number}: {episode.title}
- ë©”ì¸ í† í”½: {episode.main_topic or 'ì—†ìŒ'}

## ì¥ë©´ ì •ë³´
- ì¥ë©´ ë²ˆí˜¸: {scene.scene_number}
- ì¥ë©´ íƒ€ì…: {type_desc}
- ê°ˆë“± ìœ í˜•: {conflict_desc}

## ì°¸ì—¬ ìºë¦­í„°
{char_info}

## ì¥ë©´ ëª©í‘œ
{goal}
{learning_prompt}

## ì‘ì„± ì§€ì¹¨
1. ìœ„ì—ì„œ ë¶„ì„ëœ ê°•ì ì„ ìœ ì§€í•˜ê³ , ì§€ì ëœ ë¬¸ì œì ì€ í”¼í•´ì£¼ì„¸ìš”
2. ìºë¦­í„°ì˜ ë§íˆ¬ì™€ ì„±ê²©ì„ ì¼ê´€ë˜ê²Œ ìœ ì§€í•´ì£¼ì„¸ìš”
3. ì´ í”„ë¡œì íŠ¸ë§Œì˜ í†¤ê³¼ ë¶„ìœ„ê¸°ë¥¼ ìœ ì§€í•´ì£¼ì„¸ìš”
4. ëŒ€ì‚¬ëŠ” "ìºë¦­í„°ëª…: ëŒ€ì‚¬" í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”
5. ì§€ë¬¸ì€ (ê´„í˜¸) ì•ˆì— ì‘ì„±í•´ì£¼ì„¸ìš”
6. í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”
7. ê¸°ì¡´ ëŒ€ë³¸ë³´ë‹¤ ë” ë‚˜ì€ í’ˆì§ˆë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”

ëŒ€ë³¸ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ ëŒ€ë³¸ ì‘ê°€ì…ë‹ˆë‹¤. ê¸°ì¡´ ëŒ€ë³¸ì˜ ìŠ¤íƒ€ì¼ì„ í•™ìŠµí•˜ê³ , í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ì ì  ë” ì¢‹ì€ ëŒ€ë³¸ì„ ì‘ì„±í•©ë‹ˆë‹¤. ì¼ê´€ëœ í†¤ê³¼ ìºë¦­í„° íŠ¹ì„±ì„ ìœ ì§€í•˜ë©´ì„œë„ ë” ì°½ì˜ì ì´ê³  ì¬ë¯¸ìˆëŠ” ëŒ€ë³¸ì„ ë§Œë“­ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=2500,
            temperature=0.75
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"OpenAI ì—ëŸ¬: {e}")
        return generate_sample_script(project, characters, goal, scene_type, learning_context)


def generate_sample_script(project, characters, goal, scene_type, learning_context=None):
    """OpenAI ì—†ì„ ë•Œ ìƒ˜í”Œ ëŒ€ë³¸ ìƒì„± (í•™ìŠµ ì»¨í…ìŠ¤íŠ¸ ë°˜ì˜)"""
    
    char_names = [c.name for c in characters] if characters else ["ì§„í–‰ìA", "ì§„í–‰ìB"]
    
    if len(char_names) < 2:
        char_names = char_names + ["ì§„í–‰ì"]
    
    # í•™ìŠµëœ ê°•ì  ë°˜ì˜
    style_note = ""
    if learning_context and learning_context.get("strengths_to_keep"):
        style_note = f"(ìŠ¤íƒ€ì¼ ì°¸ê³ : {', '.join(learning_context['strengths_to_keep'][:2])})"
    
    scripts = {
        "opening": f"""{char_names[0]}: (ì¹´ë©”ë¼ë¥¼ ë³´ë©° ì—ë„ˆì§€ ë„˜ì¹˜ê²Œ) ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ë„ ì°¾ì•„ì™”ìŠµë‹ˆë‹¤.

{char_names[1]}: ë„¤, ì˜¤ëŠ˜ì€ ì •ë§ í¥ë¯¸ë¡œìš´ ì£¼ì œê°€ ìˆì–´ìš”.

{char_names[0]}: ë§ì•„ìš”. {goal}

{char_names[1]}: (ì‹œì²­ìë¥¼ í–¥í•´) ì˜¤ëŠ˜ ë°©ì†¡ ëë‚˜ë©´ ì´ ì£¼ì œ, ì™„ë²½í•˜ê²Œ ì´í•´í•˜ì‹¤ ìˆ˜ ìˆì„ ê±°ì˜ˆìš”.

{char_names[0]}: ì, ê·¸ëŸ¼ ë°”ë¡œ ì‹œì‘í•´ë³¼ê¹Œìš”?

{char_names[1]}: ë„¤, ë“¤ì–´ê°‘ë‹ˆë‹¤!

{style_note}""",

        "dialogue": f"""{char_names[0]}: ì´ ë¶€ë¶„ì´ í•µì‹¬ì¸ ê²ƒ ê°™ì•„ìš”.

{char_names[1]}: ë§ì•„ìš”. {goal}

{char_names[0]}: ê·¸ëŸ°ë° ì—¬ê¸°ì„œ ì¤‘ìš”í•œ í¬ì¸íŠ¸ê°€ ìˆì–´ìš”.

{char_names[1]}: (ë„ë•ì´ë©°) ê·¸ ë¶€ë¶„ì´ ì •ë§ ì¤‘ìš”í•˜ì£ .

{char_names[0]}: ì¢€ ë” ìì„¸íˆ í’€ì–´ë³¼ê²Œìš”.

{char_names[1]}: (ìë£Œë¥¼ ë³´ë©°) ì—¬ê¸° ë³´ì‹œë©´...

{char_names[0]}: ì•„, ì´ê²Œ ë°”ë¡œ ê·¸ê±°ë„¤ìš”!

{style_note}""",

        "talk": f"""{char_names[0]}: ì, ë³¸ê²©ì ìœ¼ë¡œ ë“¤ì–´ê°€ ë³´ê² ìŠµë‹ˆë‹¤.

{char_names[1]}: {goal}

{char_names[0]}: ì´ê²Œ ì™œ ì¤‘ìš”í•˜ëƒë©´ìš”...

{char_names[1]}: ë§¥ë½ì„ ë³´ë©´ ì´í•´ê°€ ë©ë‹ˆë‹¤.

{char_names[0]}: (ìë£Œë¥¼ ë³´ë©°) ì—¬ê¸° ë³´ì‹œë©´ ëª…í™•í•´ìš”.

{char_names[1]}: ì•„, ê·¸ë ‡êµ°ìš”. ì´ê²Œ í•µì‹¬ í¬ì¸íŠ¸ë„¤ìš”.

{char_names[0]}: ì‹œì²­ìë¶„ë“¤ë„ ì´ì œ ì´í•´ë˜ì…¨ì£ ?

{style_note}""",

        "highlight": f"""{char_names[0]}: (ê°•ì¡°í•˜ë©°) ì—¬ê¸°ê°€ ì˜¤ëŠ˜ì˜ í•µì‹¬ì…ë‹ˆë‹¤!

{char_names[1]}: {goal}

{char_names[0]}: ì´ê±´ ê¼­ ê¸°ì–µí•˜ì…”ì•¼ í•´ìš”.

{char_names[1]}: (ë¼ì–´ë“¤ë©°) ì •ë§ ì¤‘ìš”í•œ ë¶€ë¶„ì´ì—ìš”.

{char_names[0]}: ë‹¤ì‹œ í•œë²ˆ ì •ë¦¬í•˜ë©´...

{char_names[1]}: ë°”ë¡œ ì´ê±°ì˜ˆìš”!

{char_names[0]}: ì‹œì²­ì ì—¬ëŸ¬ë¶„, ì´ê±° ë†“ì¹˜ì‹œë©´ ì•ˆ ë©ë‹ˆë‹¤!

{style_note}""",

        "closing": f"""{char_names[0]}: ì, ì˜¤ëŠ˜ ë‚´ìš©ì„ ì •ë¦¬í•´ ë³´ë©´ìš”.

{char_names[1]}: ì²«ì§¸, {goal}

{char_names[0]}: ì´ê²Œ ì˜¤ëŠ˜ì˜ í•µì‹¬ì´ì—ˆìŠµë‹ˆë‹¤.

{char_names[1]}: ë‹¤ìŒ ì£¼ì—ëŠ” ë” ì¬ë¯¸ìˆëŠ” ì£¼ì œë¡œ ì°¾ì•„ëµ™ê² ìŠµë‹ˆë‹¤.

{char_names[0]}: ì‹œì²­í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤!

(í•¨ê»˜) ë‹¤ìŒì— ë˜ ë§Œë‚˜ìš”!

{style_note}"""
    }
    
    return scripts.get(scene_type, scripts["dialogue"])


async def auto_evaluate_scene(db: Session, client, scene, content: str, learning_context: dict) -> dict:
    """ì¥ë©´ ìƒì„± í›„ ìë™ìœ¼ë¡œ í‰ê°€ ìˆ˜í–‰"""
    
    # ê¸°ì¡´ í‰ê°€ê°€ ìˆìœ¼ë©´ ì‚­ì œ
    existing_eval = db.query(EvaluationModel).filter(
        EvaluationModel.scene_id == scene.id
    ).first()
    if existing_eval:
        db.delete(existing_eval)
        db.commit()
    
    if client:
        # OpenAIë¡œ í‰ê°€
        eval_result = await evaluate_with_openai(client, content, learning_context)
    else:
        # ìƒ˜í”Œ í‰ê°€ ìƒì„±
        eval_result = generate_sample_evaluation(content, learning_context)
    
    # í‰ê°€ ì €ì¥
    evaluation = EvaluationModel(
        scene_id=scene.id,
        creativity_score=eval_result["creativity_score"],
        consistency_score=eval_result["consistency_score"],
        emotion_score=eval_result["emotion_score"],
        pacing_score=eval_result["pacing_score"],
        dialogue_score=eval_result["dialogue_score"],
        overall_score=eval_result["overall_score"],
        cliche_detected=eval_result["cliche_detected"],
        cliches=eval_result.get("cliches", []),
        issues=eval_result.get("issues", []),
        summary=eval_result["summary"],
        suggestions=eval_result.get("suggestions", []),
        strengths=eval_result.get("strengths", []),
        evaluator_model="auto"
    )
    db.add(evaluation)
    db.commit()
    
    return {
        "overall_score": eval_result["overall_score"],
        "evaluated": True
    }


async def evaluate_with_openai(client, content: str, learning_context: dict) -> dict:
    """OpenAIë¡œ ëŒ€ë³¸ í‰ê°€"""
    
    prompt = f"""ë‹¤ìŒ ëŒ€ë³¸ì„ í‰ê°€í•´ì£¼ì„¸ìš”. JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.

ëŒ€ë³¸:
```
{content[:1500]}
```

í‰ê°€ ê¸°ì¤€:
1. creativity_score (0.0~1.0): ì°½ì˜ì„±, ì‹ ì„ í•¨
2. consistency_score (0.0~1.0): ì¼ê´€ì„±, ìºë¦­í„° ìœ ì§€
3. emotion_score (0.0~1.0): ê°ì • ì „ë‹¬ë ¥
4. pacing_score (0.0~1.0): í˜ì´ì‹±, ë¦¬ë“¬
5. dialogue_score (0.0~1.0): ëŒ€í™” ìì—°ìŠ¤ëŸ¬ì›€

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "creativity_score": 0.0,
    "consistency_score": 0.0,
    "emotion_score": 0.0,
    "pacing_score": 0.0,
    "dialogue_score": 0.0,
    "overall_score": 0.0,
    "cliche_detected": false,
    "cliches": [],
    "strengths": ["ê°•ì 1", "ê°•ì 2"],
    "suggestions": ["ì œì•ˆ1", "ì œì•ˆ2"],
    "summary": "ì „ì²´ í‰ê°€ ìš”ì•½"
}}"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ëŒ€ë³¸ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=800,
            temperature=0.3
        )
        
        import json
        result_text = response.choices[0].message.content
        # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
        if "```json" in result_text:
            result_text = result_text.split("```json")[1].split("```")[0]
        elif "```" in result_text:
            result_text = result_text.split("```")[1].split("```")[0]
        
        return json.loads(result_text.strip())
    except Exception as e:
        print(f"í‰ê°€ ì—ëŸ¬: {e}")
        return generate_sample_evaluation(content, learning_context)


def generate_sample_evaluation(content: str, learning_context: dict) -> dict:
    """ìƒ˜í”Œ í‰ê°€ ìƒì„±"""
    
    # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± í‰ê°€
    word_count = len(content)
    line_count = content.count('\n') + 1
    dialogue_count = content.count(':')
    
    # ê¸°ë³¸ ì ìˆ˜ (0.7~0.9)
    import random
    base = 0.75 + random.random() * 0.15
    
    # ëŒ€í™”ê°€ ë§ìœ¼ë©´ ëŒ€í™” ì ìˆ˜ ë†’ì„
    dialogue_bonus = min(0.1, dialogue_count * 0.02)
    
    return {
        "creativity_score": round(base + random.random() * 0.1, 2),
        "consistency_score": round(base + 0.05, 2),
        "emotion_score": round(base + random.random() * 0.08, 2),
        "pacing_score": round(base + random.random() * 0.05, 2),
        "dialogue_score": round(base + dialogue_bonus, 2),
        "overall_score": round(base + 0.03, 2),
        "cliche_detected": random.random() < 0.2,
        "cliches": [],
        "strengths": ["ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” íë¦„", "ìºë¦­í„° ë§íˆ¬ ìœ ì§€"],
        "suggestions": ["ê°ì • í‘œí˜„ì„ ë” ë‹¤ì–‘í•˜ê²Œ"],
        "summary": f"ì´ {word_count}ì, {dialogue_count}ê°œ ëŒ€ì‚¬. ì „ë°˜ì ìœ¼ë¡œ ì–‘í˜¸í•œ ëŒ€ë³¸ì…ë‹ˆë‹¤."
    }


@router.get("/context/{project_id}", summary="í•™ìŠµ ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ")
async def get_learning_context(
    project_id: int,
    db: Session = Depends(get_db)
):
    """
    í”„ë¡œì íŠ¸ì˜ í•™ìŠµëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    AIê°€ ì–´ë–¤ ì •ë³´ë¥¼ í•™ìŠµí–ˆëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    project = get_project(db, project_id)
    if not project:
        raise HTTPException(status_code=404, detail="í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    context = build_learning_context(db, project_id)
    
    return {
        "project_id": project_id,
        "project_title": project.title,
        "learning_summary": {
            "total_scenes_analyzed": context["scene_count"],
            "average_score": round(context["avg_score"], 3),
            "strengths_learned": context["strengths_to_keep"],
            "issues_to_avoid": context["issues_to_avoid"],
            "character_patterns": {
                name: len(examples) 
                for name, examples in context["character_examples"].items()
            }
        },
        "best_scenes_count": len(context["best_scenes"]),
        "ready_for_generation": context["scene_count"] > 0
    }
